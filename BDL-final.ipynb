{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big data lab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import my_func\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set()\n",
    "\n",
    "fe_name_tr = 'churn_train_model_fe.pickle'\n",
    "b_num_name_tr ='churn_train_model_b_num.pickle'\n",
    "dpi_name_tr ='churn_train_model_dpi.pickle'\n",
    "\n",
    "fe_name_te = 'churn_test_model_fe.pickle'\n",
    "b_num_name_te ='churn_test_model_b_num.pickle'\n",
    "dpi_name_te ='churn_test_model_dpi.pickle'\n",
    "\n",
    "file_path = r'D:\\BDL-Final\\data-from-BDL' + '\\\\'\n",
    "\n",
    "# settings for dataframe\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframes\n",
    "df_train_fe = pd.read_pickle(file_path+fe_name_tr)\n",
    "df_train_dpi = pd.read_pickle(file_path+dpi_name_tr)\n",
    "df_train_b_num = pd.read_pickle(file_path+b_num_name_tr)\n",
    "\n",
    "df_test_fe = pd.read_pickle(file_path+fe_name_te)\n",
    "df_test_dpi = pd.read_pickle(file_path+dpi_name_te)\n",
    "df_test_b_num = pd.read_pickle(file_path+b_num_name_te)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {   'objective': 'binary',\n",
    "                'metric': 'auc',\n",
    "                'num_boost_round': 500,\n",
    "                'is_unbalance': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out how data looks like\n",
    "# find unique values of applications\n",
    "#len(df_train_fe.columns)\n",
    "# df_apli_tr = df_train_dpi['Application'].unique()\n",
    "# df_apli_te = df_test_dpi['Application'].unique()\n",
    "#df_exel = df_train_dpi.iloc[0:1000000]\n",
    "#df_exel.to_excel('dpi.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apl_te= pd.DataFrame(df_apli_te)\n",
    "# apl_tr= pd.DataFrame(df_apli_tr)\n",
    "\n",
    "# #len(df_apli_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train_fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def binary_classification_metrics(y_true_tr, y_pred_tr, y_true_val = None, y_pred_val = None, report = False):\n",
    "    print (\"{:<15} {:<10} {:<10} {:<10}\".format('Metrics','Train','Test','\\u0394'))\n",
    "    metrics_dict = {}\n",
    "\n",
    "    metrics_dict['roc_auc'] = np.round(roc_auc_score(y_true_tr, y_pred_tr),4)\n",
    "    metrics_dict['accuracy'] = np.round(accuracy_score(y_true_tr, y_pred_tr),4)\n",
    "    metrics_dict['precision'] = np.round(precision_score(y_true_tr, y_pred_tr),4)\n",
    "    metrics_dict['recall'] = np.round(recall_score(y_true_tr, y_pred_tr),4)\n",
    "    metrics_dict['f1_score'] = np.round(f1_score(y_true_tr, y_pred_tr),4)\n",
    "    \n",
    "    if y_true_val is not None:\n",
    "        metrics_dict_test = {}\n",
    "        metrics_dict_test['roc_auc'] = np.round(roc_auc_score(y_true_val, y_pred_val),4)\n",
    "        metrics_dict_test['accuracy'] = np.round(accuracy_score(y_true_val, y_pred_val),4)\n",
    "        metrics_dict_test['precision'] = np.round(precision_score(y_true_val, y_pred_val),4)\n",
    "        metrics_dict_test['recall'] = np.round(recall_score(y_true_val, y_pred_val),4)\n",
    "        metrics_dict_test['f1_score'] = np.round(f1_score(y_true_val, y_pred_val),4)\n",
    "        # print(classification_report(y_true_val, y_pred_val))\n",
    "    \n",
    "        for metrics, value in metrics_dict.items():\n",
    "            value_test = metrics_dict_test[metrics]\n",
    "            diff = np.round(metrics_dict_test[metrics] - value, 4)\n",
    "            print (\"{:<15} {:<10} {:<10} {:<10}\".format(metrics, value, value_test, diff))\n",
    "    else:\n",
    "        for metrics, value in metrics_dict.items():\n",
    "            print (\"{:<15} {:<10}\".format(metrics, value))\n",
    "    if report:\n",
    "        print('\\n')\n",
    "        print('Train :')\n",
    "        print(classification_report(y_true_tr, y_pred_tr))\n",
    "        if not y_true_val.empty:\n",
    "            print('Test :')\n",
    "            print(classification_report(y_true_val, y_pred_val))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06389333333333333"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# перевірка на missing значення\n",
    "ms_val = my_func.count_missing_values(df_train_fe)\n",
    "ms_val"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing dataset for base modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset for training\n",
    "def ds_base_prep(df):\n",
    "    # missing change for -1\n",
    "    df.fillna(-1, inplace=True)\n",
    "\n",
    "    # remove abon_id\n",
    "    return(df)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr = ds_base_prep(df_train_fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_tr['target']\n",
    "X = df_tr.drop(['target', 'abon_id'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=777, shuffle=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model = None, features = []):\n",
    "    name = str(model.__class__).split('.')[-1][:-2] + '_' + datetime.today().strftime(\"%d%m%Y_%H_%M\") + '.pickle'    \n",
    "    if model:\n",
    "        with open(name, 'wb') as file:\n",
    "            pickle.dump((model, features), file)\n",
    "            print('Save', name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on Test sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = df_test_fe['target']\n",
    "X_test = df_test_fe.drop(['target', 'abon_id'], axis=1)\n",
    "X_test.replace(np.inf, -1, inplace=True)\n",
    "X_test.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lgb = lgb_model.predict(X_train)\n",
    "pred_lgb_test = lgb_model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binary_classification_metrics(y, clf.predict(X), y_test, clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_auc(y_train, y_train_pred, y_test, y_test_pred, title):\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_train, y_train_pred)\n",
    "    auc = roc_auc_score(y_train, y_train_pred)\n",
    "\n",
    "    plt.plot(fpr, tpr, color='blue',\n",
    "                 label=\"Навчальна вибірка, AUC={:.3f}\".format(auc), linestyle='-')\n",
    "\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test,  y_test_pred)\n",
    "    auc = roc_auc_score(y_test, y_test_pred)\n",
    "\n",
    "    plt.plot(fpr, tpr, color='black',\n",
    "                 label=\"Тестова вибірка, AUC={:.3f}\".format(auc), linestyle='--')\n",
    "\n",
    "\n",
    "\n",
    "    plt.plot([0,1], [0,1], color='gray', linestyle='--')\n",
    "\n",
    "    plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
    "    plt.xlabel(\"Flase Positive Rate\", fontsize=15)\n",
    "\n",
    "    plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
    "    plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
    "\n",
    "    plt.title(title, fontweight='bold', fontsize=15)\n",
    "    plt.legend(prop={'size':13}, loc='lower right')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_auc(y_train, pred_lgb, y_test, pred_lgb_test, \"LightGBM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
